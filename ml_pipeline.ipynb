{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap --no-stderr\n",
    "!pip install pyarrow\n",
    "!pip install spacy\n",
    "!pip install pandarallel\n",
    "!pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/commit_messages.parquet\n",
      "./data/commit_messages_with_label.parquet\n",
      "... ./data/labeled_commit_message_train.parquet\n",
      "... ./data/labeled_commit_message_dev.parquet\n",
      "... ./data/labeled_commit_message_test.parquet\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"./data/\"\n",
    "data_path = os.path.join(data_dir, \"commit_messages.parquet\")\n",
    "tagged_data_path = os.path.join(data_dir, \"commit_messages_with_label.parquet\")\n",
    "tagged_trainset_path = os.path.join(data_dir, \"labeled_commit_message_train.parquet\")\n",
    "tagged_devset_path = os.path.join(data_dir, \"labeled_commit_message_dev.parquet\")\n",
    "tagged_testset_path = os.path.join(data_dir, \"labeled_commit_message_test.parquet\")\n",
    "print(data_path)\n",
    "print(tagged_data_path)\n",
    "print(\"...\", tagged_trainset_path)\n",
    "print(\"...\", tagged_devset_path)\n",
    "print(\"...\", tagged_testset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(562500, 11) (187500, 11) (250000, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>time_sec</th>\n",
       "      <th>subject</th>\n",
       "      <th>message</th>\n",
       "      <th>first_segment_message</th>\n",
       "      <th>identifier</th>\n",
       "      <th>length_ok</th>\n",
       "      <th>capital_first_token</th>\n",
       "      <th>not_period_end</th>\n",
       "      <th>imperative_mood</th>\n",
       "      <th>good_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Magenik</td>\n",
       "      <td>1421858642</td>\n",
       "      <td>Timeline update</td>\n",
       "      <td>Timeline update\\n</td>\n",
       "      <td>Timeline update\\n</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>polytomous</td>\n",
       "      <td>1509246803</td>\n",
       "      <td>Extract parameter on breakCurrentBlock TF2John</td>\n",
       "      <td>Extract parameter on breakCurrentBlock TF2John\\n</td>\n",
       "      <td>Extract parameter on breakCurrentBlock TF2John\\n</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mero</td>\n",
       "      <td>1348444108</td>\n",
       "      <td>updated changelog</td>\n",
       "      <td>updated changelog\\n</td>\n",
       "      <td>updated changelog\\n</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>YominCarr</td>\n",
       "      <td>1414094992</td>\n",
       "      <td>Cleanup</td>\n",
       "      <td>Cleanup\\n</td>\n",
       "      <td>Cleanup\\n</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anthony Fuentes</td>\n",
       "      <td>1556550569</td>\n",
       "      <td>[maint] Updating copyright</td>\n",
       "      <td>[maint] Updating copyright\\n</td>\n",
       "      <td>[maint] Updating copyright\\n</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name    time_sec  \\\n",
       "0          Magenik  1421858642   \n",
       "1       polytomous  1509246803   \n",
       "2             mero  1348444108   \n",
       "3        YominCarr  1414094992   \n",
       "4  Anthony Fuentes  1556550569   \n",
       "\n",
       "                                          subject  \\\n",
       "0                                 Timeline update   \n",
       "1  Extract parameter on breakCurrentBlock TF2John   \n",
       "2                               updated changelog   \n",
       "3                                         Cleanup   \n",
       "4                      [maint] Updating copyright   \n",
       "\n",
       "                                            message  \\\n",
       "0                                 Timeline update\\n   \n",
       "1  Extract parameter on breakCurrentBlock TF2John\\n   \n",
       "2                               updated changelog\\n   \n",
       "3                                         Cleanup\\n   \n",
       "4                      [maint] Updating copyright\\n   \n",
       "\n",
       "                              first_segment_message  identifier  length_ok  \\\n",
       "0                                 Timeline update\\n        True       True   \n",
       "1  Extract parameter on breakCurrentBlock TF2John\\n       False       True   \n",
       "2                               updated changelog\\n        True       True   \n",
       "3                                         Cleanup\\n       False       True   \n",
       "4                      [maint] Updating copyright\\n        True       True   \n",
       "\n",
       "   capital_first_token  not_period_end  imperative_mood  good_message  \n",
       "0                 True            True            False         False  \n",
       "1                 True            True             True         False  \n",
       "2                False            True             True         False  \n",
       "3                 True            True            False         False  \n",
       "4                False            True             True         False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_set = pd.read_parquet(tagged_trainset_path)\n",
    "dev_set = pd.read_parquet(tagged_devset_path)\n",
    "test_set = pd.read_parquet(tagged_testset_path)\n",
    "print(train_set.shape, dev_set.shape, test_set.shape)\n",
    "display(train_set.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering (Training Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/commit_messages.parquet\n",
      "./data/commit_messages_with_label.parquet\n",
      "... ./data/labeled_commit_message_train.parquet\n",
      "... ./data/labeled_commit_message_dev.parquet\n",
      "... ./data/labeled_commit_message_test.parquet\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"./data/\"\n",
    "data_path = os.path.join(data_dir, \"commit_messages.parquet\")\n",
    "tagged_data_path = os.path.join(data_dir, \"commit_messages_with_label.parquet\")\n",
    "tagged_trainset_path = os.path.join(data_dir, \"labeled_commit_message_train.parquet\")\n",
    "tagged_devset_path = os.path.join(data_dir, \"labeled_commit_message_dev.parquet\")\n",
    "tagged_testset_path = os.path.join(data_dir, \"labeled_commit_message_test.parquet\")\n",
    "print(data_path)\n",
    "print(tagged_data_path)\n",
    "print(\"...\", tagged_trainset_path)\n",
    "print(\"...\", tagged_devset_path)\n",
    "print(\"...\", tagged_testset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(562500, 11) (187500, 11) (250000, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>time_sec</th>\n",
       "      <th>subject</th>\n",
       "      <th>message</th>\n",
       "      <th>first_segment_message</th>\n",
       "      <th>identifier</th>\n",
       "      <th>length_ok</th>\n",
       "      <th>capital_first_token</th>\n",
       "      <th>not_period_end</th>\n",
       "      <th>imperative_mood</th>\n",
       "      <th>good_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Magenik</td>\n",
       "      <td>1421858642</td>\n",
       "      <td>Timeline update</td>\n",
       "      <td>Timeline update\\n</td>\n",
       "      <td>Timeline update\\n</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>polytomous</td>\n",
       "      <td>1509246803</td>\n",
       "      <td>Extract parameter on breakCurrentBlock TF2John</td>\n",
       "      <td>Extract parameter on breakCurrentBlock TF2John\\n</td>\n",
       "      <td>Extract parameter on breakCurrentBlock TF2John\\n</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mero</td>\n",
       "      <td>1348444108</td>\n",
       "      <td>updated changelog</td>\n",
       "      <td>updated changelog\\n</td>\n",
       "      <td>updated changelog\\n</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>YominCarr</td>\n",
       "      <td>1414094992</td>\n",
       "      <td>Cleanup</td>\n",
       "      <td>Cleanup\\n</td>\n",
       "      <td>Cleanup\\n</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anthony Fuentes</td>\n",
       "      <td>1556550569</td>\n",
       "      <td>[maint] Updating copyright</td>\n",
       "      <td>[maint] Updating copyright\\n</td>\n",
       "      <td>[maint] Updating copyright\\n</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name    time_sec  \\\n",
       "0          Magenik  1421858642   \n",
       "1       polytomous  1509246803   \n",
       "2             mero  1348444108   \n",
       "3        YominCarr  1414094992   \n",
       "4  Anthony Fuentes  1556550569   \n",
       "\n",
       "                                          subject  \\\n",
       "0                                 Timeline update   \n",
       "1  Extract parameter on breakCurrentBlock TF2John   \n",
       "2                               updated changelog   \n",
       "3                                         Cleanup   \n",
       "4                      [maint] Updating copyright   \n",
       "\n",
       "                                            message  \\\n",
       "0                                 Timeline update\\n   \n",
       "1  Extract parameter on breakCurrentBlock TF2John\\n   \n",
       "2                               updated changelog\\n   \n",
       "3                                         Cleanup\\n   \n",
       "4                      [maint] Updating copyright\\n   \n",
       "\n",
       "                              first_segment_message  identifier  length_ok  \\\n",
       "0                                 Timeline update\\n        True       True   \n",
       "1  Extract parameter on breakCurrentBlock TF2John\\n       False       True   \n",
       "2                               updated changelog\\n        True       True   \n",
       "3                                         Cleanup\\n       False       True   \n",
       "4                      [maint] Updating copyright\\n        True       True   \n",
       "\n",
       "   capital_first_token  not_period_end  imperative_mood  good_message  \n",
       "0                 True            True            False         False  \n",
       "1                 True            True             True         False  \n",
       "2                False            True             True         False  \n",
       "3                 True            True            False         False  \n",
       "4                False            True             True         False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_set = pd.read_parquet(tagged_trainset_path)\n",
    "dev_set = pd.read_parquet(tagged_devset_path)\n",
    "test_set = pd.read_parquet(tagged_testset_path)\n",
    "print(train_set.shape, dev_set.shape, test_set.shape)\n",
    "display(train_set.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(562500, 2) (562500, 3) (562500,)\n"
     ]
    }
   ],
   "source": [
    "train_set_text_features = train_set.loc[:, [\"subject\", \"first_segment_message\"]]\n",
    "train_set_nontext_features = train_set.loc[:, [\"length_ok\", \"capital_first_token\", \"not_period_end\"]]\n",
    "train_set_labels = train_set.loc[:, \"good_message\"]\n",
    "print(train_set_text_features.shape, train_set_nontext_features.shape, train_set_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer Learn to fit on feature col <subject> ... \n",
      "Tokenizer Learn to fit on feature col <first_segment_message> ... \n",
      "Tokenizer contains 243430 unique tokens...\n",
      "[0]: <Token: timeline> --> <ID: 280>\n",
      "[1]: <Token: update> --> <ID: 110756>\n",
      "[2]: <Token: extract> --> <ID: 1075>\n",
      "[3]: <Token: parameter> --> <ID: 2622>\n",
      "[4]: <Token: on> --> <ID: 36728>\n",
      "Tokenizer Started to Convert Text Features to Sequences ...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=100000, lower=True, oov_token=\"<unk>\")\n",
    "\n",
    "for col in train_set_text_features.columns:\n",
    "    print(f\"Tokenizer Learn to fit on feature col <{col}> ... \")\n",
    "    focus_texts = train_set_text_features[col]\n",
    "    tokenizer.fit_on_texts(focus_texts)\n",
    "\n",
    "print(f\"Tokenizer contains {tokenizer.word_counts.items().__len__()} unique tokens...\")\n",
    "_ = [print(f\"[{idx}]: <Token: {kv[0]}> --> <ID: {kv[1]}>\") for idx, kv in enumerate(tokenizer.word_counts.items()) if idx < 5]\n",
    "\n",
    "print(\"Tokenizer Started to Convert Text Features to Sequences ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Feature Engineering (Transform Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Working on train set\n",
      "(562500, 2) (562500, 3) (562500,)\n",
      "Transforming the text features <subject> ... \n",
      "Transforming the text features <first_segment_message> ... \n",
      "[0]: <Key: subject> --> <Shape of Attributes: (562500, 200)>\n",
      "[1]: <Key: first_segment_message> --> <Shape of Attributes: (562500, 200)>\n",
      "[2]: <Key: nontext_features> --> <Shape of Attributes: (562500, 3)>\n",
      "(562500, 403) (562500, 1) (562500, 404)\n",
      "\n",
      ">>> Working on dev set\n",
      "(187500, 2) (187500, 3) (187500,)\n",
      "Transforming the text features <subject> ... \n",
      "Transforming the text features <first_segment_message> ... \n",
      "[0]: <Key: subject> --> <Shape of Attributes: (187500, 200)>\n",
      "[1]: <Key: first_segment_message> --> <Shape of Attributes: (187500, 200)>\n",
      "[2]: <Key: nontext_features> --> <Shape of Attributes: (187500, 3)>\n",
      "(187500, 403) (187500, 1) (187500, 404)\n",
      "\n",
      ">>> Working on test set\n",
      "(250000, 2) (250000, 3) (250000,)\n",
      "Transforming the text features <subject> ... \n",
      "Transforming the text features <first_segment_message> ... \n",
      "[0]: <Key: subject> --> <Shape of Attributes: (250000, 200)>\n",
      "[1]: <Key: first_segment_message> --> <Shape of Attributes: (250000, 200)>\n",
      "[2]: <Key: nontext_features> --> <Shape of Attributes: (250000, 3)>\n",
      "(250000, 403) (250000, 1) (250000, 404)\n"
     ]
    }
   ],
   "source": [
    "TEXT_FEATURE_COLUMNS = [\"subject\", \"first_segment_message\"]\n",
    "NONTEXT_FEATURE_COLUMNS = [\"length_ok\", \"capital_first_token\", \"not_period_end\"]\n",
    "LABEL_COLUMNS = \"good_message\"\n",
    "PRESET_TOKENIZER = tokenizer\n",
    "MAX_LEN = 200\n",
    "\n",
    "for focus_set_tag, focus_set in zip([\"train\", \"dev\", \"test\"], [train_set, dev_set, test_set]):\n",
    "    print(f\"\\n>>> Working on {focus_set_tag} set\")\n",
    "    \n",
    "    focus_set_text_features = focus_set.loc[:, TEXT_FEATURE_COLUMNS]\n",
    "    focus_set_nontext_features = focus_set.loc[:, NONTEXT_FEATURE_COLUMNS]\n",
    "    focus_set_labels = focus_set.loc[:, LABEL_COLUMNS]\n",
    "    print(focus_set_text_features.shape, focus_set_nontext_features.shape, focus_set_labels.shape)\n",
    "\n",
    "    \n",
    "    TRANSFORMED_FEATURES_DICT = {\"subject\": None, \"first_segment_message\": None, \"nontext_features\": None}\n",
    "\n",
    "    for col in focus_set_text_features.columns:\n",
    "        print(f\"Transforming the text features <{col}> ... \")\n",
    "        focus_texts = focus_set_text_features[col]\n",
    "        TRANSFORMED_FEATURES_DICT[col] = pad_sequences(tokenizer.texts_to_sequences(focus_set_text_features[\"subject\"]), maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "    focus_set_nontext_features = focus_set_nontext_features.astype(\"int32\").values\n",
    "    TRANSFORMED_FEATURES_DICT[\"nontext_features\"] = focus_set_nontext_features\n",
    "\n",
    "    _ = [print(f\"[{idx}]: <Key: {kv[0]}> --> <Shape of Attributes: {kv[1].shape}>\") for idx, kv in enumerate(TRANSFORMED_FEATURES_DICT.items()) if idx < 5]\n",
    "    \n",
    "    CONCATENATED_FEATURES = np.hstack([v for v in TRANSFORMED_FEATURES_DICT.values()])\n",
    "    CONCATENATED_LABELS = focus_set_labels.astype(\"int32\").values.reshape(-1, 1)\n",
    "    CONCATENATED_DATASET = pd.DataFrame(np.hstack([CONCATENATED_LABELS, CONCATENATED_FEATURES]))\n",
    "    print(CONCATENATED_FEATURES.shape, CONCATENATED_LABELS.shape, CONCATENATED_DATASET.shape)\n",
    "    CONCATENATED_DATASET.to_csv(f\"data/encoded/encoded_{focus_set_tag}_set.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Localized Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Lambda, GlobalMaxPooling1D, Concatenate\n",
    "from tensorflow.keras.metrics import Accuracy\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimpleNet(input_dim=403, embedding_vocab_size=100000, embedding_dim=32, sequence_size=200):\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "\n",
    "    text_x = Lambda(lambda x: x[:, : sequence_size * 2])(input_layer)\n",
    "    rule_x = Lambda(lambda x: x[:, sequence_size * 2 - input_dim :])(input_layer)\n",
    "\n",
    "    text_emb = Embedding(embedding_vocab_size, embedding_dim)(text_x)\n",
    "\n",
    "    text_pool = GlobalMaxPooling1D()(text_emb)\n",
    "\n",
    "    concat_x = Concatenate()([text_pool, rule_x])\n",
    "\n",
    "    x = Dense(32, activation='relu')(concat_x)\n",
    "    output_layer = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Training Data Will be Acquired From: ./data/encoded/encoded_train_set.csv\n",
      "... Validation Data Will be Acquired From: ./data/encoded/encoded_dev_set.csv\n",
      "Successfully Load Data, Shape of the features: (562500, 403)\n",
      "Successfully Load Data, Shape of the features: (187500, 403)\n",
      "[[2361    5    0 ...    1    1    1]\n",
      " [ 870  368   21 ...    1    1    1]\n",
      " [  29  187    0 ...    1    0    1]\n",
      " [ 109    0    0 ...    1    1    1]\n",
      " [2141  124  884 ...    1    0    1]]\n",
      "[0 0 0 0 0]\n",
      "[[   3   15   14 ...    1    1    1]\n",
      " [   7  186  743 ...    1    0    1]\n",
      " [ 445  338  276 ...    1    1    1]\n",
      " [   5  676    0 ...    1    1    1]\n",
      " [ 261    2 4321 ...    1    0    1]]\n"
     ]
    }
   ],
   "source": [
    "def acquire_inputs(input_data_dict):\n",
    "    \"\"\"\n",
    "    acquire the training set feature, label and validation set feature, label\n",
    "    wrap them into the proper manner that fit into the model training procedure\n",
    "    \"\"\"\n",
    "    \n",
    "    train_data_path = os.path.join(input_data_dict['train'], \"encoded_train_set.csv\")\n",
    "    validation_data_path = os.path.join(input_data_dict['validation'], \"encoded_dev_set.csv\")\n",
    "    print(\"... Training Data Will be Acquired From: {}\".format(train_data_path))\n",
    "    print(\"... Validation Data Will be Acquired From: {}\".format(validation_data_path))\n",
    "    encoded_trainset = pd.read_csv(train_data_path, header=None)\n",
    "    encoded_devset = pd.read_csv(validation_data_path, header=None)\n",
    "\n",
    "    encoded_trainset_feature = encoded_trainset.iloc[:, 1:].values\n",
    "    encoded_trainset_label = encoded_trainset.iloc[:, 0].values\n",
    "    encoded_trainset = None\n",
    "    print(\"Successfully Load Data, Shape of the features:\", encoded_trainset_feature.shape)\n",
    "\n",
    "    encoded_devset_feature = encoded_devset.iloc[:, 1:].values\n",
    "    encoded_devset_label = encoded_devset.iloc[:, 0].values\n",
    "    encoded_devset = None\n",
    "    print(\"Successfully Load Data, Shape of the features:\", encoded_devset_feature.shape)\n",
    "    return encoded_trainset_feature, encoded_trainset_label, (encoded_devset_feature, encoded_devset_label)\n",
    "\n",
    "encoded_data_dir = \"./data/encoded\"\n",
    "channel_input_dirs = {\"train\": encoded_data_dir, \"validation\": encoded_data_dir, \"test\": encoded_data_dir}\n",
    "x_train, y_train, validation_data = acquire_inputs(channel_input_dirs)\n",
    "print(x_train[:5])\n",
    "print(y_train[:5])\n",
    "print(validation_data[0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           [(None, 403)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 400)          0           input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 400, 32)      3200000     lambda_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 32)           0           embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_23 (Lambda)              (None, 3)            0           input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 35)           0           global_max_pooling1d_11[0][0]    \n",
      "                                                                 lambda_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 32)           1152        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 1)            33          dense_22[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,201,185\n",
      "Trainable params: 3,201,185\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model will be saved to ./model.hdf5 ...\n",
      "Train on 562500 samples, validate on 187500 samples\n",
      "Epoch 1/3\n",
      "562432/562500 [============================>.] - ETA: 0s - loss: 0.1213 - acc: 0.9559\n",
      "Epoch 00001: val_loss improved from inf to 0.06501, saving model to ./model.hdf5\n",
      "562500/562500 [==============================] - 68s 121us/sample - loss: 0.1213 - acc: 0.9559 - val_loss: 0.0650 - val_acc: 0.9725\n",
      "Epoch 2/3\n",
      "562176/562500 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9765\n",
      "Epoch 00002: val_loss improved from 0.06501 to 0.06175, saving model to ./model.hdf5\n",
      "562500/562500 [==============================] - 68s 121us/sample - loss: 0.0572 - acc: 0.9765 - val_loss: 0.0618 - val_acc: 0.9735\n",
      "Epoch 3/3\n",
      "562432/562500 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9808\n",
      "Epoch 00003: val_loss did not improve from 0.06175\n",
      "562500/562500 [==============================] - 67s 120us/sample - loss: 0.0476 - acc: 0.9808 - val_loss: 0.0625 - val_acc: 0.9734\n"
     ]
    }
   ],
   "source": [
    "# Provided train function\n",
    "def train(model, train_data_feature, train_data_label, validation_data, epochs=10, model_dir=None, verbose=1, batch_size=128):\n",
    "    \"\"\"\n",
    "    This is the training method that is called by the tensorflow training script\n",
    "    \"\"\"\n",
    "    model.summary()\n",
    "    \n",
    "    model_path = os.path.join(model_dir, \"model.hdf5\")\n",
    "    print(f\"Model will be saved to {model_path} ...\")\n",
    "    \n",
    "    ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "        model_path, monitor='val_loss', verbose=1, save_best_only=True, \n",
    "        mode='auto', save_freq='epoch', options=None\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        x=train_data_feature, y=train_data_label, \n",
    "        validation_data=validation_data,\n",
    "        epochs=epochs, verbose=verbose,\n",
    "        shuffle=True, batch_size=batch_size,\n",
    "        callbacks = [ckpt]\n",
    "    )\n",
    "        \n",
    "EMBEDDING_DIM = 32\n",
    "EMBEDDING_VOCAB_SIZE = 100000\n",
    "TOTAL_FEATURE_DIM = 403\n",
    "SEQUENCE_SIZE = 200\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 3\n",
    "\n",
    "model = SimpleNet(input_dim = TOTAL_FEATURE_DIM, \n",
    "                  embedding_vocab_size = EMBEDDING_VOCAB_SIZE,\n",
    "                  embedding_dim = EMBEDDING_DIM,\n",
    "                  sequence_size = SEQUENCE_SIZE)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
    "\n",
    "model_dir = \"./\"\n",
    "train(model, x_train, y_train, validation_data, epochs=3, model_dir=model_dir, verbose=1, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           [(None, 403)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 400)          0           input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 400, 32)      3200000     lambda_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 32)           0           embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_23 (Lambda)              (None, 3)            0           input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 35)           0           global_max_pooling1d_11[0][0]    \n",
      "                                                                 lambda_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 32)           1152        concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 1)            33          dense_22[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,201,185\n",
      "Trainable params: 3,201,185\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_path = \"model.hdf5\"\n",
    "model_reload = tf.keras.models.load_model(model_path)\n",
    "model_reload.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagemaker Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset: ./data/encoded/encoded_train_set.csv\n",
      "Devset: ./data/encoded/encoded_dev_set.csv\n",
      "Testset: ./data/encoded/encoded_test_set.csv\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"./data/encoded\"\n",
    "encoded_trainset_path = os.path.join(data_dir, \"encoded_train_set.csv\")\n",
    "encoded_devset_path = os.path.join(data_dir, \"encoded_dev_set.csv\")\n",
    "encoded_testset_path = os.path.join(data_dir, \"encoded_test_set.csv\")\n",
    "print(\"Trainset:\", encoded_trainset_path)\n",
    "print(\"Devset:\", encoded_devset_path)\n",
    "print(\"Testset:\", encoded_testset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using role: arn:aws:iam::259046265119:role/service-role/AmazonSageMaker-ExecutionRole-20200813T153989\n",
      "Using bucket: sagemaker-us-east-1-259046265119\n"
     ]
    }
   ],
   "source": [
    "# SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "print(f\"Using role: {role}\")\n",
    "\n",
    "# default S3 bucket\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "print(f\"Using bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train]: ./data/encoded/encoded_train_set.csv --> s3://sagemaker-us-east-1-259046265119/commit_msg_data/train/encoded_train_set.csv\n",
      "[validation]: ./data/encoded/encoded_dev_set.csv --> s3://sagemaker-us-east-1-259046265119/commit_msg_data/validation/encoded_dev_set.csv\n",
      "[test]: ./data/encoded/encoded_test_set.csv --> s3://sagemaker-us-east-1-259046265119/commit_msg_data/test/encoded_test_set.csv\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': 's3://sagemaker-us-east-1-259046265119/commit_msg_data/train/encoded_train_set.csv',\n",
       " 'validation': 's3://sagemaker-us-east-1-259046265119/commit_msg_data/validation/encoded_dev_set.csv',\n",
       " 'test': 's3://sagemaker-us-east-1-259046265119/commit_msg_data/test/encoded_test_set.csv'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# upload to S3\n",
    "tags = [\"train\", \"validation\", \"test\"]\n",
    "upload_paths = [encoded_trainset_path, encoded_devset_path, encoded_testset_path]\n",
    "input_data = {}\n",
    "\n",
    "for tag, upload_path in zip(tags, upload_paths):\n",
    "    prefix = f'commit_msg_data/{tag}'\n",
    "    input_data[tag] = sagemaker_session.upload_data(path=upload_path, bucket=bucket, key_prefix=prefix)\n",
    "    print(f\"[{tag}]: {upload_path} --> {input_data[tag]}\")\n",
    "\n",
    "print('')\n",
    "display(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commit_msg_data/test/encoded_test_set.csv\n",
      "commit_msg_data/train/encoded_train_set.csv\n",
      "commit_msg_data/validation/encoded_dev_set.csv\n"
     ]
    }
   ],
   "source": [
    "for obj in boto3.resource('s3').Bucket(bucket).objects.all():\n",
    "    if any([identifier in obj.key for identifier in [\"csv\"]]):\n",
    "         print(obj.key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling: Create Tensorflow Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model artifact will be output to: s3://sagemaker-us-east-1-259046265119/commit_msg_model\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "# specify an output path\n",
    "# prefix is specified above\n",
    "\n",
    "output_prefix=\"commit_msg_model\"\n",
    "output_path = 's3://{}/{}'.format(bucket, output_prefix)\n",
    "print(f\"Model artifact will be output to: {output_path}\")\n",
    "\n",
    "# instantiate a pytorch estimator\n",
    "estimator = TensorFlow(entry_point='train.py',\n",
    "                       source_dir='codes', # this should be just \"source\" for your code\n",
    "                       role=role,\n",
    "                       framework_version='2.1',\n",
    "                       train_instance_count=1,\n",
    "                       train_instance_type='ml.m5.xlarge',\n",
    "#                        output_path=output_path,\n",
    "                       sagemaker_session=sagemaker_session,\n",
    "                       py_version=\"py3\",\n",
    "                       hyperparameters={\n",
    "                           'epochs': 5, # could change to higher\n",
    "                           'batch_size': 128\n",
    "                       })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-27 17:21:13 Starting - Starting the training job...\n",
      "2020-11-27 17:21:15 Starting - Launching requested ML instances......\n",
      "2020-11-27 17:22:31 Starting - Preparing the instances for training...\n",
      "2020-11-27 17:23:08 Downloading - Downloading input data...\n",
      "2020-11-27 17:23:36 Training - Downloading the training image..\u001b[34m2020-11-27 17:23:54,837 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2020-11-27 17:23:54,845 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\n",
      "2020-11-27 17:23:51 Training - Training image download completed. Training in progress.\u001b[34m2020-11-27 17:24:05,626 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-11-27 17:24:05,642 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-11-27 17:24:05,656 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-11-27 17:24:05,665 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 128,\n",
      "        \"model_dir\": \"s3://sagemaker-us-east-1-259046265119/tensorflow-training-2020-11-27-17-21-12-971/model\",\n",
      "        \"epochs\": 5\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"tensorflow-training-2020-11-27-17-21-12-971\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-259046265119/tensorflow-training-2020-11-27-17-21-12-971/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":128,\"epochs\":5,\"model_dir\":\"s3://sagemaker-us-east-1-259046265119/tensorflow-training-2020-11-27-17-21-12-971/model\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-259046265119/tensorflow-training-2020-11-27-17-21-12-971/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":128,\"epochs\":5,\"model_dir\":\"s3://sagemaker-us-east-1-259046265119/tensorflow-training-2020-11-27-17-21-12-971/model\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tensorflow-training-2020-11-27-17-21-12-971\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-259046265119/tensorflow-training-2020-11-27-17-21-12-971/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"128\",\"--epochs\",\"5\",\"--model_dir\",\"s3://sagemaker-us-east-1-259046265119/tensorflow-training-2020-11-27-17-21-12-971/model\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=128\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=s3://sagemaker-us-east-1-259046265119/tensorflow-training-2020-11-27-17-21-12-971/model\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=5\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python3 train.py --batch_size 128 --epochs 5 --model_dir s3://sagemaker-us-east-1-259046265119/tensorflow-training-2020-11-27-17-21-12-971/model\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m============== Parsed Args ===============: \u001b[0m\n",
      "\u001b[34mNamespace(batch_size=128, current_host='algo-1', data_dir='/opt/ml/input/data/train', embedding_dim=32, embedding_vocab_size=100000, epochs=5, hosts=['algo-1'], model_dir='s3://sagemaker-us-east-1-259046265119/tensorflow-training-2020-11-27-17-21-12-971/model', sequence_size=200, total_feature_dim=403, train_env={'additional_framework_parameters': {}, 'channel_input_dirs': {'test': '/opt/ml/input/data/test', 'train': '/opt/ml/input/data/train', 'validation': '/opt/ml/input/data/validation'}, 'current_host': 'algo-1', 'framework_module': 'sagemaker_tensorflow_container.training:main', 'hosts': ['algo-1'], 'hyperparameters': {'batch_size': 128, 'epochs': 5, 'model_dir': 's3://sagemaker-us-east-1-259046265119/tensorflow-training-2020-11-27-17-21-12-971/model'}, 'input_config_dir': '/opt/ml/input/config', 'input_data_config': {'test': {'RecordWrapperType': 'None', 'S3DistributionType': 'FullyReplicated', 'TrainingInputMode': 'File'}, 'train': {'RecordWrapperType': 'None', 'S3DistributionType': 'FullyReplicated', 'TrainingInputMode': 'File'}, 'validation': {'RecordWrapperType': 'None', 'S3DistributionType': 'FullyReplicated', 'TrainingInputMode': 'File'}}, 'input_dir': '/opt/ml/input', 'is_master': True, 'job_name': 'tensorflow-training-2020-11-27-17-21-12-971', 'log_level': 20, 'master_hostname': 'algo-1', 'model_dir': '/opt/ml/model', 'module_dir': 's3://sagemaker-us-east-1-259046265119/tensorflow-training-2020-11-27-17-21-12-971/source/sourcedir.tar.gz', 'module_name': 'train', 'network_interface_name': 'eth0', 'num_cpus': 4, 'num_gpus': 0, 'output_data_dir': '/opt/ml/output/data', 'output_dir': '/opt/ml/output', 'output_intermediate_dir': '/opt/ml/output/intermediate', 'resource_config': {'current_host': 'algo-1', 'hosts': ['algo-1'], 'network_interface_name': 'eth0'}, 'user_entry_point': 'train.py'})\u001b[0m\n",
      "\u001b[34m[2020-11-27 17:24:07.632 ip-10-0-145-149.ec2.internal:26 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-11-27 17:24:07.633 ip-10-0-145-149.ec2.internal:26 INFO hook.py:193] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-11-27 17:24:07.633 ip-10-0-145-149.ec2.internal:26 INFO hook.py:238] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2020-11-27 17:24:07.633 ip-10-0-145-149.ec2.internal:26 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2020-11-27 17:24:07.663 ip-10-0-145-149.ec2.internal:26 INFO hook.py:398] Monitoring the collections: metrics, losses, sm_metrics\u001b[0m\n",
      "\u001b[34mModel: \"model\"\u001b[0m\n",
      "\u001b[34m__________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mLayer (type)                    Output Shape         Param #     Connected to                     \u001b[0m\n",
      "\u001b[34m==================================================================================================\u001b[0m\n",
      "\u001b[34minput_1 (InputLayer)            [(None, 403)]        0                                            \u001b[0m\n",
      "\u001b[34m__________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mlambda (Lambda)                 (None, 400)          0           input_1[0][0]                    \u001b[0m\n",
      "\u001b[34m__________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34membedding (Embedding)           (None, 400, 32)      3200000     lambda[0][0]                     \u001b[0m\n",
      "\u001b[34m__________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mglobal_max_pooling1d (GlobalMax (None, 32)           0           embedding[0][0]                  \u001b[0m\n",
      "\u001b[34m__________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mlambda_1 (Lambda)               (None, 3)            0           input_1[0][0]                    \u001b[0m\n",
      "\u001b[34m__________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mconcatenate (Concatenate)       (None, 35)           0           global_max_pooling1d[0][0]       \n",
      "                                                                 lambda_1[0][0]                   \u001b[0m\n",
      "\u001b[34m__________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mdense (Dense)                   (None, 32)           1152        concatenate[0][0]                \u001b[0m\n",
      "\u001b[34m__________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mdense_1 (Dense)                 (None, 1)            33          dense[0][0]                      \u001b[0m\n",
      "\u001b[34m==================================================================================================\u001b[0m\n",
      "\u001b[34mTotal params: 3,201,185\u001b[0m\n",
      "\u001b[34mTrainable params: 3,201,185\u001b[0m\n",
      "\u001b[34mNon-trainable params: 0\u001b[0m\n",
      "\u001b[34m__________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34m... Training Data Will be Acquired From: /opt/ml/input/data/train/encoded_train_set.csv\u001b[0m\n",
      "\u001b[34m... Validation Data Will be Acquired From: /opt/ml/input/data/validation/encoded_dev_set.csv\u001b[0m\n",
      "\u001b[34mSuccessfully Load Data, Shape of the features: (562500, 403)\u001b[0m\n",
      "\u001b[34mSuccessfully Load Data, Shape of the features: (187500, 403)\u001b[0m\n",
      "\u001b[34mModel: \"model\"\u001b[0m\n",
      "\u001b[34m__________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mLayer (type)                    Output Shape         Param #     Connected to                     \u001b[0m\n",
      "\u001b[34m==================================================================================================\u001b[0m\n",
      "\u001b[34minput_1 (InputLayer)            [(None, 403)]        0                                            \u001b[0m\n",
      "\u001b[34m__________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mlambda (Lambda)                 (None, 400)          0           input_1[0][0]                    \u001b[0m\n",
      "\u001b[34m__________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34membedding (Embedding)           (None, 400, 32)      3200000     lambda[0][0]                     \u001b[0m\n",
      "\u001b[34m__________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mglobal_max_pooling1d (GlobalMax (None, 32)           0           embedding[0][0]                  \u001b[0m\n",
      "\u001b[34m__________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mlambda_1 (Lambda)               (None, 3)            0           input_1[0][0]                    \u001b[0m\n",
      "\u001b[34m__________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mconcatenate (Concatenate)       (None, 35)           0           global_max_pooling1d[0][0]       \n",
      "                                                                 lambda_1[0][0]                   \u001b[0m\n",
      "\u001b[34m__________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mdense (Dense)                   (None, 32)           1152        concatenate[0][0]                \u001b[0m\n",
      "\u001b[34m__________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mdense_1 (Dense)                 (None, 1)            33          dense[0][0]                      \u001b[0m\n",
      "\u001b[34m==================================================================================================\u001b[0m\n",
      "\u001b[34mTotal params: 3,201,185\u001b[0m\n",
      "\u001b[34mTrainable params: 3,201,185\u001b[0m\n",
      "\u001b[34mNon-trainable params: 0\u001b[0m\n",
      "\u001b[34m__________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mModel will be saved to s3://sagemaker-us-east-1-259046265119/tensorflow-training-2020-11-27-17-21-12-971/model/model.hdf5 ...\u001b[0m\n",
      "\u001b[34mTrain on 562500 samples, validate on 187500 samples\u001b[0m\n",
      "\u001b[34mEpoch 1/5\u001b[0m\n",
      "\u001b[34mEpoch 00001: val_loss improved from inf to 0.06566, saving model to s3://sagemaker-us-east-1-259046265119/tensorflow-training-2020-11-27-17-21-12-971/model/model.hdf5\u001b[0m\n",
      "\u001b[34m2020-11-27 17:26:26,899 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mCommand \"/usr/bin/python3 train.py --batch_size 128 --epochs 5 --model_dir s3://sagemaker-us-east-1-259046265119/tensorflow-training-2020-11-27-17-21-12-971/model\"\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py\", line 1029, in _save_model\n",
      "    self.model.save(filepath, overwrite=True)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py\", line 1008, in save\n",
      "    signatures, options)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/save.py\", line 112, in save_model\n",
      "    model, filepath, overwrite, include_optimizer)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/hdf5_format.py\", line 92, in save_model_to_hdf5\n",
      "    f = h5py.File(filepath, mode='w')\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\", line 408, in __init__\n",
      "    swmr=swmr)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\", line 179, in make_fid\n",
      "    fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl)\n",
      "  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n",
      "  File \"h5py/h5f.pyx\", line 108, in h5py.h5f.create\u001b[0m\n",
      "\u001b[34mOSError: Unable to create file (unable to open file: name = 's3://sagemaker-us-east-1-259046265119/tensorflow-training-2020-11-27-17-21-12-971/model/model.hdf5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)\n",
      "\u001b[0m\n",
      "\u001b[34mDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"train.py\", line 125, in <module>\n",
      "    train(model, x_train, y_train, validation_data, args.epochs, args.model_dir, 2, args.batch_size)\n",
      "  File \"train.py\", line 45, in train\n",
      "    callbacks = [ckpt]\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\", line 825, in fit\n",
      "    use_multiprocessing=use_multiprocessing)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 397, in fit\n",
      "    prefix='val_')\n",
      "  File \"/usr/lib/python3.6/contextlib.py\", line 88, in __exit__\n",
      "    next(self.gen)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 771, in on_epoch\n",
      "    self.callbacks.on_epoch_end(epoch, epoch_logs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py\", line 302, in on_epoch_end\n",
      "    callback.on_epoch_end(epoch, logs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py\", line 992, in on_epoch_end\n",
      "    self._save_model(epoch=epoch, logs=logs)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py\", line 1045, in _save_model\n",
      "    if 'is a directory' in e.message:\u001b[0m\n",
      "\u001b[34mAttributeError: 'OSError' object has no attribute 'message'\u001b[0m\n",
      "\n",
      "2020-11-27 17:26:36 Uploading - Uploading generated training model\n",
      "2020-11-27 17:26:36 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job tensorflow-training-2020-11-27-17-21-12-971: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/usr/bin/python3 train.py --batch_size 128 --epochs 5 --model_dir s3://sagemaker-us-east-1-259046265119/tensorflow-training-2020-11-27-17-21-12-971/model\"\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py\", line 1029, in _save_model\n    self.model.save(filepath, overwrite=True)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py\", line 1008, in save\n    signatures, options)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/save.py\", line 112, in save_model\n    model, filepath, overwrite, include_optimizer)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/hdf5_format.py\", line 92, in save_model_to_hdf5\n    f = h5py.File(filepath, mode='w')\n  File \"/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\", line 408, in __init__\n    swmr=swmr)\n  File \"/usr/local/lib/python3.6/dist-pac",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-58df39faa653>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1208\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1210\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1211\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3365\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3366\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3367\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   2955\u001b[0m                 ),\n\u001b[1;32m   2956\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2957\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2958\u001b[0m             )\n\u001b[1;32m   2959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job tensorflow-training-2020-11-27-17-21-12-971: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/usr/bin/python3 train.py --batch_size 128 --epochs 5 --model_dir s3://sagemaker-us-east-1-259046265119/tensorflow-training-2020-11-27-17-21-12-971/model\"\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/callbacks.py\", line 1029, in _save_model\n    self.model.save(filepath, overwrite=True)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/network.py\", line 1008, in save\n    signatures, options)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/save.py\", line 112, in save_model\n    model, filepath, overwrite, include_optimizer)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/saving/hdf5_format.py\", line 92, in save_model_to_hdf5\n    f = h5py.File(filepath, mode='w')\n  File \"/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\", line 408, in __init__\n    swmr=swmr)\n  File \"/usr/local/lib/python3.6/dist-pac"
     ]
    }
   ],
   "source": [
    "estimator.fit(inputs = input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
